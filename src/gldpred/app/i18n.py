"""
Internationalization (i18n) module for the Streamlit application.

All user-facing strings are stored here, keyed by language code.
To add a new language, add a new top-level key (e.g. ``"fr"``) and
provide translations for every entry.

Usage::

    from gldpred.app.i18n import STRINGS
    t = STRINGS["en"]          # or STRINGS["es"]
    st.header(t["data_header"])
"""

LANGUAGES = {"English": "en", "EspaÃ±ol": "es"}

# ---------------------------------------------------------------------------
# Master translation dictionary
# ---------------------------------------------------------------------------
STRINGS: dict[str, dict[str, str]] = {
    # ======================================================================
    # ENGLISH
    # ======================================================================
    "en": {
        # -- Page chrome ----------------------------------------------------
        "page_title": "GLD Price Prediction",
        "app_title": "ğŸ… GLD Price Prediction with Deep Learning",
        "app_subtitle": "Forecast Gold ETF price movements using GRU / LSTM / TCN models",

        # -- Sidebar --------------------------------------------------------
        "sidebar_config": "Configuration",
        "sidebar_language": "Language / Idioma",
        "sidebar_data_settings": "Data Settings",
        "sidebar_start_date": "Start Date",
        "sidebar_end_date": "End Date",
        "sidebar_model_settings": "Model Settings",
        "sidebar_model_arch": "Model Architecture",
        "sidebar_task_type": "Task Type",
        "sidebar_task_regression": "Regression (Returns)",
        "sidebar_task_classification": "Classification (Buy/No-Buy)",
        "sidebar_task_multitask": "Multi-task (Reg + Cls)",
        "sidebar_horizon": "Prediction Horizon (days)",
        "sidebar_training_settings": "Training Settings",
        "sidebar_seq_length": "Sequence Length",
        "sidebar_hidden_size": "Hidden Size",
        "sidebar_num_layers": "Number of Layers",
        "sidebar_epochs": "Epochs",
        "sidebar_batch_size": "Batch Size",
        "sidebar_learning_rate": "Learning Rate",
        "sidebar_buy_threshold": "Buy Threshold",
        "sidebar_w_reg": "Regression Loss Weight",
        "sidebar_w_cls": "Classification Loss Weight",
        "sidebar_about": "About",
        "sidebar_about_text": (
            "This application uses deep learning (GRU / LSTM / TCN) to predict "
            "GLD price movements. It supports regression, classification, and "
            "multi-task learning at multiple time horizons (1, 5, 20 days) "
            "with automatic training diagnostics."
        ),

        # -- Tab names ------------------------------------------------------
        "tab_data": "ğŸ“Š Data",
        "tab_train": "ğŸ”§ Train Model",
        "tab_predictions": "ğŸ“ˆ Predictions",
        "tab_evaluation": "ğŸ“‰ Evaluation",
        "tab_tutorial": "ğŸ“š Tutorial",

        # -- Tab 1: Data ----------------------------------------------------
        "data_header": "Data Loading and Exploration",
        "data_load_btn": "Load Data",
        "data_loading_spinner": "Loading GLD data...",
        "data_load_success": "âœ… Loaded {n} records from {start} to {end}",
        "data_load_error": "âŒ Error loading data: {err}",
        "data_metric_records": "Records",
        "data_metric_price": "Latest Price",
        "data_metric_change": "Price Change",
        "data_metric_features": "Features",
        "data_price_history": "Price History",
        "data_preview": "Data Preview",
        "data_info": (
            "GLD historical data is fetched from Yahoo Finance (yfinance). "
            "The table shows OHLCV columns (Open, High, Low, Close, Volume) "
            "plus 28 engineered features such as moving averages, RSI, MACD, "
            "volatility measures, and lag features. These help the model "
            "detect patterns not visible in raw prices."
        ),

        # -- Tab 2: Training ------------------------------------------------
        "train_header": "Model Training",
        "train_warn_no_data": "âš ï¸ Please load data first in the Data tab",
        "train_btn": "Train Model",
        "train_spinner": "Training model...",
        "train_complete": "Training complete!",
        "train_success": "âœ… Model trained successfully! Saved to {path}",
        "train_error": "âŒ Error training model: {err}",
        "train_history": "Training History",
        "train_loss_label": "Train Loss",
        "val_loss_label": "Validation Loss",
        "train_xlabel": "Epoch",
        "train_ylabel": "Loss",
        "train_plot_title": "Training and Validation Loss",
        "train_info": (
            "Clicking 'Train Model' runs a full training loop: feature "
            "selection â†’ target computation â†’ sequence creation â†’ 80/20 "
            "train/validation split â†’ gradient-descent optimisation. "
            "Watch the loss plot: both curves should decrease. If validation "
            "loss rises while training loss falls, the model is overfitting."
        ),

        # -- Tab 3: Predictions ---------------------------------------------
        "pred_header": "Model Predictions",
        "pred_warn_no_model": "âš ï¸ Please train a model first in the Train Model tab",
        "pred_vs_actual": "Predictions vs Actual",
        "pred_returns": "**Predicted Returns:**",
        "pred_implied": "**Implied Price Movements:**",
        "pred_signals": "**Buy/No-Buy Signals:**",
        "pred_actual_returns": "Actual Returns",
        "pred_predicted_returns": "Predicted Returns",
        "pred_actual_price": "Actual Price",
        "pred_implied_price": "Implied Price (from prediction)",
        "pred_actual_signal": "Actual Signal",
        "pred_predicted_signal": "Predicted Signal",
        "pred_recent": "Recent Predictions",
        "pred_error": "âŒ Error making predictions: {err}",
        "pred_col_date": "Date",
        "pred_col_price": "Actual Price",
        "pred_col_pred": "Prediction",
        "pred_col_true": "True Value",
        "pred_info": (
            "After training, the model runs a forward pass on every input "
            "sequence. For regression the output is the predicted return; "
            "for classification it is a Buy (>0.5) or No-Buy (â‰¤0.5) "
            "probability. The 'Implied Price' chart multiplies the actual "
            "price by (1 + predicted return). These are historical "
            "predictions, not true future forecasts."
        ),

        # -- Tab 4: Evaluation ----------------------------------------------
        "eval_header": "Model Evaluation",
        "eval_warn_no_model": "âš ï¸ Please train a model first in the Train Model tab",
        "eval_regression_metrics": "Regression Metrics",
        "eval_classification_metrics": "Classification Metrics",
        "eval_confusion_matrix": "Confusion Matrix",
        "eval_cm_no_buy": "No-Buy",
        "eval_cm_buy": "Buy",
        "eval_cm_title": "Confusion Matrix",
        "eval_cm_ylabel": "True label",
        "eval_cm_xlabel": "Predicted label",
        "eval_detailed": "Detailed Metrics",
        "eval_error": "âŒ Error evaluating model: {err}",
        "eval_info": (
            "Regression metrics: MSE, RMSE, MAE measure prediction error "
            "(lower is better); RÂ² measures variance explained (1.0 = "
            "perfect). Classification metrics: Accuracy is the fraction "
            "correct, but Precision, Recall, and F1 are more informative "
            "because a naive 'always Buy' model can still reach ~60% "
            "accuracy. The confusion matrix shows TP/FP/FN/TN counts."
        ),

        # -- Diagnostics panel -----------------------------------------------
        "diag_header": "Training Diagnostics",
        "diag_verdict": "Verdict",
        "diag_verdict_healthy": "âœ… Healthy",
        "diag_verdict_overfitting": "âš ï¸ Overfitting",
        "diag_verdict_underfitting": "âš ï¸ Underfitting",
        "diag_verdict_noisy": "âš ï¸ Noisy / Unstable",
        "diag_explanation": "Explanation",
        "diag_suggestions": "Suggestions",
        "diag_best_epoch": "Best Epoch",
        "diag_gen_gap": "Generalization Gap",

        # -- Multi-task prediction labels ----------------------------------------
        "pred_mt_returns": "**Regression Head â€” Predicted Returns:**",
        "pred_mt_signals": "**Classification Head â€” Buy/No-Buy Signals:**",
        "pred_mt_col_reg": "Predicted Return",
        "pred_mt_col_cls": "Buy Probability",

        # -- Multi-task evaluation labels ----------------------------------------
        "eval_mt_header": "Multi-task Evaluation",
        "eval_mt_threshold": "Classification Threshold",

        # -- Axis labels (plots) -------------------------------------------
        "axis_date": "Date",
        "axis_price": "Price (USD)",
        "axis_returns": "Returns",
        "axis_signal": "Signal (1=Buy, 0=No-Buy)",

        # -- Tutorial -------------------------------------------------------
        "tut_header": "ğŸ“š Tutorial â€” How This Application Works",
        "tut_disclaimer": (
            "> **Disclaimer:** This application is an educational tool for "
            "exploring deep learning applied to financial time series. "
            "Nothing in this guide or in the application's output constitutes "
            "financial advice."
        ),
        "tut_s1_title": "1 â€” Overview",
        "tut_s1_body": """
This application downloads historical price data for the **GLD** exchange-traded
fund (Gold ETF), engineers a set of technical features from that data, and then
trains a deep-learning model to **predict future price movements**.

The workflow follows four steps, each represented by a tab in the UI:

| Tab | Purpose |
|-----|---------|
| **ğŸ“Š Data** | Download and explore GLD historical prices |
| **ğŸ”§ Train Model** | Configure and train a neural network |
| **ğŸ“ˆ Predictions** | Visualise the model's forecasts |
| **ğŸ“‰ Evaluation** | Measure the model's accuracy with standard metrics |

The sidebar on the left lets you configure every parameter before pressing
*Train Model*.
""",
        "tut_s2_title": "2 â€” Data: Loading & Exploration",
        "tut_s2_body": """
### How data is loaded

GLD historical data is fetched via **yfinance**, a Python library that retrieves
daily market data from Yahoo Finance. When you press *Load Data*, the app
downloads daily OHLCV (Open, High, Low, Close, Volume) records for the date
range configured in the sidebar.

### What each column represents

| Column | Meaning |
|--------|---------|
| **Open** | The price at market open for the day |
| **High** | The highest price reached during the day |
| **Low** | The lowest price reached during the day |
| **Close** | The price at market close â€” the most commonly used reference |
| **Volume** | The total number of shares traded that day |
| **Dividends** | Cash dividends paid (usually 0 for GLD) |
| **Stock Splits** | Split events (usually 0 for GLD) |

### Feature engineering

Raw OHLCV data alone is not very informative for a neural network.
The application automatically creates **28 additional features** before
training, including:

- **Moving averages** (SMA, EMA at 5, 10, 20, 50 days) â€” smoothed trend lines
- **Volatility measures** â€” rolling standard deviation of returns
- **Momentum indicators** â€” rate of price change over different windows
- **RSI (Relative Strength Index)** â€” measures if the asset is overbought or
  oversold (range 0â€“100)
- **MACD (Moving Average Convergence Divergence)** â€” trend-following momentum
  indicator
- **Volume ratios** â€” how today's volume compares to recent averages
- **Lag features** â€” previous days' prices and returns fed as explicit inputs

These features help the model detect **patterns and regime changes** that are
not visible in raw price data.
""",
        "tut_s3_title": "3 â€” Model Architectures: GRU vs LSTM vs TCN",
        "tut_s3_body": """
### What are Recurrent Neural Networks (RNNs)?

Standard neural networks treat every input independently. **Recurrent neural
networks** (RNNs) are designed to process *sequences*: they maintain an internal
**hidden state** that is updated at each time step, allowing the model to
remember information from earlier in the sequence.

This makes RNNs naturally suited for **time-series data** such as stock prices,
where the order of observations matters.

### GRU (Gated Recurrent Unit)

The GRU is a modern RNN variant introduced in 2014. It uses two *gates*:

- **Reset gate** â€” decides how much past information to forget
- **Update gate** â€” decides how much new information to let in

GRUs are **simpler and faster** to train than LSTMs because they have fewer
parameters.

### LSTM (Long Short-Term Memory)

The LSTM, introduced in 1997, uses three gates:

- **Forget gate** â€” decides what to discard from the cell state
- **Input gate** â€” decides which new values to store
- **Output gate** â€” decides what part of the cell state to output

LSTMs have a separate **cell state** in addition to the hidden state, which
allows them to retain information over **longer sequences** more effectively.

### TCN (Temporal Convolutional Network)

A **TCN** replaces recurrence with stacked 1-D *causal convolutions*.
Key properties:

- **Causal padding** â€” the model can only see past timesteps, never the
  future, preserving temporal causality.
- **Dilated filters** â€” each layer doubles the dilation factor, so the
  receptive field grows *exponentially* with depth. This lets the network
  capture long-range dependencies efficiently.
- **Residual connections** â€” skip connections inside each block prevent
  gradient degradation in deep stacks.

Because convolutions run in parallel across the sequence (no sequential
hidden-state dependency), TCNs **train faster** than RNNs on modern GPUs.

### When to choose which?

| Criterion | GRU | LSTM | TCN |
|-----------|-----|------|-----|
| Speed | âš¡ Fast | ğŸ¢ Slower | âš¡âš¡ Fastest |
| Parameters | Fewer | More | Medium |
| Short sequences (â‰¤ 30) | âœ… Sufficient | âœ… Works well | âœ… Good |
| Long sequences (> 60) | âš ï¸ May struggle | âœ… Better retention | âœ… Large receptive field |
| Limited data | âœ… Less overfitting | âš ï¸ More overfitting | âœ… Weight sharing |
| Parallelism | âŒ Sequential | âŒ Sequential | âœ… Fully parallel |

**Rule of thumb:** Start with GRU. Switch to LSTM for very long sequences
or TCN when training speed matters.

### Task types

This application supports three prediction tasks:

**Regression (Returns)**
- The model outputs a **continuous number** representing the expected
  percentage return over the prediction horizon.
- Example output: `0.012` â†’ the model expects a +1.2 % price increase.

**Classification (Buy / No-Buy)**
- The model outputs a **probability** between 0 and 1.
- If the output is > 0.5, the signal is "**Buy**" (class 1).
- If the output is â‰¤ 0.5, the signal is "**No-Buy**" (class 0).

**Multi-task (Regression + Classification)**
- A single shared backbone feeds *two* prediction heads simultaneously.
- The regression head predicts returns; the classification head predicts
  buy/no-buy signals.
- Loss: *L = w_reg Ã— MSE + w_cls Ã— BCEWithLogits*, configurable via the
  sidebar sliders.
- Benefit: the shared representation learns richer features because it must
  satisfy both objectives at once.
""",
        "tut_s4_title": "4 â€” Prediction Horizons: 1, 5 & 20 Days",
        "tut_s4_body": """
The **prediction horizon** is the number of trading days into the future
that the model tries to forecast.

| Horizon | Meaning | Character |
|---------|---------|-----------|
| **1 day** | Predict tomorrow's return / signal | Short-term, noisier |
| **5 days** | Predict the return over the next week | Medium-term balance |
| **20 days** | Predict the return over the next month | Longer-term, smoother |

### Trade-offs

- **Short horizons (1 day)** capture rapid market movements but are dominated
  by **noise** (random daily fluctuations). Models can learn spurious patterns
  and may show lower accuracy.
- **Long horizons (20 days)** smooth out noise, but future events become
  **harder to predict** because more external factors can intervene.
- **Medium horizons (5 days)** often offer a good balance for learning
  meaningful patterns without excessive noise.

### Practical advice

- If you see very erratic prediction plots, try a longer horizon (5 or 20).
- If the model seems overly smoothed and never reacts, try a shorter horizon.
- Compare the evaluation metrics across horizons to find the sweet spot for
  your data range.
""",
        "tut_s5_title": "5 â€” Configurable Parameters Explained",
        "tut_s5_body": """
Every parameter in the sidebar affects how the model learns. Below is a
guide to each one.

---

#### Sequence Length (Lookback Window)
*Sidebar: 10â€“60, default 20*

The number of **consecutive days** the model looks at before making a
prediction.

| Value | Effect |
|-------|--------|
| Small (10) | Less context; faster training; may miss longer trends |
| Large (40â€“60) | More context; slower; risk of overfitting on small datasets |

**Recommended start:** 20.

---

#### Hidden Size
*Sidebar: 32â€“128, default 64*

The number of **internal neurons** in each recurrent layer â€” controls
the model's capacity.

| Value | Effect |
|-------|--------|
| Small (32) | Simpler; faster; less overfitting risk; may underfit |
| Large (128) | More expressive; slower; higher overfitting risk |

**Recommended start:** 64.

---

#### Number of Layers
*Sidebar: 1â€“4, default 2*

Stacked recurrent layers learn **hierarchical patterns**.

| Value | Effect |
|-------|--------|
| 1 | Simple and fast |
| 2 | Good default; multi-scale patterns |
| 3â€“4 | More powerful; needs more data |

**Recommended start:** 2.

---

#### Epochs
*Sidebar: 10â€“200, default 50*

One epoch = the model has seen every training sample once.

| Value | Effect |
|-------|--------|
| Low (10â€“20) | May underfit |
| Medium (30â€“80) | Good range |
| High (100â€“200) | Risk of overfitting |

**Tip:** If validation loss rises while training loss falls, reduce epochs.

---

#### Batch Size
*Sidebar: 16â€“128, default 32*

Samples processed together before a weight update.

| Value | Effect |
|-------|--------|
| Small (16) | Noisier updates; slower wall-time |
| Large (64â€“128) | Smoother updates; faster per epoch |

**Recommended start:** 32.

---

#### Learning Rate
*Sidebar: 0.0001â€“0.01, default 0.001*

How much weights change per batch.

| Value | Effect |
|-------|--------|
| Too small (0.0001) | Very slow convergence |
| Good (0.0005â€“0.001) | Steady learning |
| Too large (0.01) | Unstable; loss may diverge |

**Recommended start:** 0.001.
""",
        "tut_s6_title": "6 â€” Training: What Happens When You Click 'Train'",
        "tut_s6_body": """
### The training loop

1. **Feature selection** â€” 28 engineered features; missing values filled.
2. **Target computation** â€” returns (regression) or binary labels (classification).
3. **Sequence creation** â€” sliding window of *Sequence Length* days.
4. **Train / Validation split** â€” 80 / 20 by default.
5. **Gradient-descent loop** â€” for each epoch the model trains on batches,
   then evaluates on the validation set.

### Understanding the Training History plot

- **Train Loss** (blue): error on training data.
- **Validation Loss** (orange): error on unseen data.

| Pattern | Diagnosis | Action |
|---------|-----------|--------|
| Both decrease steadily | âœ… Good convergence | Continue or stop early |
| Train â†“ val â†‘ | âš ï¸ Overfitting | â†“ epochs / complexity |
| Both stay high | âš ï¸ Underfitting | â†‘ capacity / epochs |
| Loss oscillates | âš ï¸ Unstable | â†“ learning rate |
| Flat from start | âš ï¸ Not learning | â†‘ learning rate |

### What is "loss"?

- **Regression:** Mean Squared Error (MSE).
- **Classification:** Binary Cross-Entropy.

Lower loss = better model.
""",
        "tut_s7_title": "7 â€” Predictions: Interpreting the Output",
        "tut_s7_body": """
### How predictions are generated

After training the model runs a forward pass on each input sequence
(pure inference â€” no gradient computation).

### Regression output

- Predicted return per date overlaid on actual returns.
- **Implied Price** = actual price Ã— (1 + predicted return).

### Classification output

- Probability > 0.5 â†’ **Buy (1)**; â‰¤ 0.5 â†’ **No-Buy (0)**.
- Blue dots = actual; red X = predicted.

### Recent Predictions table

| Column | Meaning |
|--------|---------|
| Date | Trading day |
| Actual Price | GLD close |
| Prediction | Raw model output |
| True Value | Actual target |

### Caveats

Predictions are on historical data (train + validation), **not** true
out-of-sample forecasts. Real-world performance may differ due to regime
changes, costs, and slippage.
""",
        "tut_s8_title": "8 â€” Evaluation: Understanding the Metrics",
        "tut_s8_body": """
### Regression metrics

| Metric | Meaning | Good values |
|--------|---------|-------------|
| **MSE** | Avg squared error | Lower is better |
| **RMSE** | âˆšMSE â€” same units as target | Lower is better |
| **MAE** | Avg absolute error | Lower is better |
| **RÂ²** | Variance explained | 1.0 = perfect; 0 = mean-level |

In real markets RÂ² of 0.01â€“0.05 can already be economically meaningful.

### Classification metrics

| Metric | Meaning |
|--------|---------|
| Accuracy | Fraction correct |
| Precision | Of Buy predictions, how many correct? |
| Recall | Of actual Buy days, how many caught? |
| F1 | Harmonic mean of Precision & Recall |

#### Confusion matrix

```
                 Predicted
              No-Buy    Buy
Actual No-Buy   TN       FP
       Buy       FN       TP
```

**Why accuracy alone is not enough:** A model that always says "Buy"
can reach ~60 % accuracy if the market is up 60 % of the time.
Precision, Recall, and F1 reveal true skill.

| Scenario | Metrics | Interpretation |
|----------|---------|----------------|
| Random guessing | RÂ²â‰ˆ0, Accâ‰ˆ50% | No skill |
| Slight edge | RÂ²â‰ˆ0.01â€“0.05, Accâ‰ˆ52â€“55% | Potentially useful |
| Strong (rare) | RÂ²>0.1, F1>0.65 | Verify not overfitting |
| Overfit | RÂ²>0.9 train only | Too good â€” check val |
""",
        "tut_s9_title": "9 â€” Practical Examples & Common Scenarios",
        "tut_s9_body": """
> **Note:** These examples are purely educational. They do NOT constitute
> financial advice.

---

**Scenario A â€” Positive prediction with buy signal**

Both models agree the price is likely to increase. This alignment increases
confidence, but does not guarantee a price rise. Check validation metrics.

---

**Scenario B â€” Validation loss rising while training loss decreases**

Classic **overfitting**. Reduce epochs, lower complexity, or add more data.

---

**Scenario C â€” Predictions fluctuate heavily**

May be overfitting to noise or learning rate too high. Try â†“ LR,
â†‘ sequence length, or longer horizon.

---

**Scenario D â€” Model always predicts the same value**

Collapsed to the mean. â†‘ capacity, â†‘ epochs, â†“ LR, or use more data.

---

**Scenario E â€” Very high accuracy on training data (95 %)**

Likely overfitting. True financial accuracy above 55â€“60 % is already good.
""",
        "tut_s10_title": "10 â€” Quick-Reference Cheat Sheet",
        "tut_s10_body": """
### Recommended starting configuration

| Parameter | Value |
|-----------|-------|
| Model | GRU |
| Task | Regression |
| Horizon | 5 days |
| Sequence Length | 20 |
| Hidden Size | 64 |
| Layers | 2 |
| Epochs | 50 |
| Batch Size | 32 |
| Learning Rate | 0.001 |

### Architecture quick comparison

| | GRU | LSTM | TCN |
|-|-----|------|-----|
| Best for | General use | Long sequences | Speed |
| # Parameters | Low | High | Medium |
| Training speed | Fast | Slow | Fastest |

### Common adjustments

| Problem | Try |
|---------|-----|
| Underfitting | â†‘ Hidden size, â†‘ Layers, â†‘ Epochs |
| Overfitting | â†“ Epochs, â†“ Hidden size, â†“ Layers, â†‘ Data range |
| Unstable loss | â†“ Learning rate |
| Flat predictions | â†‘ Learning rate, â†‘ Hidden size |
| Noisy predictions | â†‘ Sequence length, â†‘ Horizon, â†“ Learning rate |
| Slow training | â†“ Hidden size, â†“ Layers, â†‘ Batch size, TCN or GRU |

### Diagnostics verdicts

| Verdict | Meaning | Action |
|---------|---------|--------|
| âœ… Healthy | Both curves decreasing, stable gap | Continue or stop |
| âš ï¸ Overfitting | Val â†‘ while train â†“ | â†“ epochs / complexity |
| âš ï¸ Underfitting | Both curves high and flat | â†‘ capacity / epochs |
| âš ï¸ Noisy | Validation oscillates | â†“ learning rate, â†‘ batch |
""",
    },

    # ======================================================================
    # SPANISH
    # ======================================================================
    "es": {
        # -- Page chrome ----------------------------------------------------
        "page_title": "PredicciÃ³n del precio de GLD",
        "app_title": "ğŸ… PredicciÃ³n del precio de GLD con Deep Learning",
        "app_subtitle": "PronÃ³stico de movimientos del ETF de oro con modelos GRU / LSTM / TCN",

        # -- Sidebar --------------------------------------------------------
        "sidebar_config": "ConfiguraciÃ³n",
        "sidebar_language": "Language / Idioma",
        "sidebar_data_settings": "Datos",
        "sidebar_start_date": "Fecha de inicio",
        "sidebar_end_date": "Fecha de fin",
        "sidebar_model_settings": "Modelo",
        "sidebar_model_arch": "Arquitectura del modelo",
        "sidebar_task_type": "Tipo de tarea",
        "sidebar_task_regression": "RegresiÃ³n (Rendimientos)",
        "sidebar_task_classification": "ClasificaciÃ³n (Compra/No-Compra)",
        "sidebar_task_multitask": "Multi-tarea (Reg + Cls)",
        "sidebar_horizon": "Horizonte de predicciÃ³n (dÃ­as)",
        "sidebar_training_settings": "Entrenamiento",
        "sidebar_seq_length": "Longitud de secuencia",
        "sidebar_hidden_size": "TamaÃ±o oculto",
        "sidebar_num_layers": "NÃºmero de capas",
        "sidebar_epochs": "Ã‰pocas",
        "sidebar_batch_size": "TamaÃ±o de lote",
        "sidebar_learning_rate": "Tasa de aprendizaje",
        "sidebar_buy_threshold": "Umbral de compra",
        "sidebar_w_reg": "Peso de pÃ©rdida regresiÃ³n",
        "sidebar_w_cls": "Peso de pÃ©rdida clasificaciÃ³n",
        "sidebar_about": "Acerca de",
        "sidebar_about_text": (
            "Esta aplicaciÃ³n utiliza aprendizaje profundo (GRU / LSTM / TCN) "
            "para predecir movimientos del precio de GLD. Soporta regresiÃ³n, "
            "clasificaciÃ³n y aprendizaje multi-tarea en mÃºltiples horizontes "
            "temporales (1, 5, 20 dÃ­as) con diagnÃ³sticos automÃ¡ticos."
        ),

        # -- Tab names ------------------------------------------------------
        "tab_data": "ğŸ“Š Datos",
        "tab_train": "ğŸ”§ Entrenar",
        "tab_predictions": "ğŸ“ˆ Predicciones",
        "tab_evaluation": "ğŸ“‰ EvaluaciÃ³n",
        "tab_tutorial": "ğŸ“š Tutorial",

        # -- Tab 1: Data ----------------------------------------------------
        "data_header": "Carga y exploraciÃ³n de datos",
        "data_load_btn": "Cargar datos",
        "data_loading_spinner": "Cargando datos de GLD...",
        "data_load_success": "âœ… Se cargaron {n} registros desde {start} hasta {end}",
        "data_load_error": "âŒ Error al cargar datos: {err}",
        "data_metric_records": "Registros",
        "data_metric_price": "Ãšltimo precio",
        "data_metric_change": "VariaciÃ³n",
        "data_metric_features": "CaracterÃ­sticas",
        "data_price_history": "Historia de precios",
        "data_preview": "Vista previa de datos",
        "data_info": (
            "Los datos histÃ³ricos de GLD se obtienen de Yahoo Finance "
            "(yfinance). La tabla muestra columnas OHLCV (Apertura, MÃ¡ximo, "
            "MÃ­nimo, Cierre, Volumen) mÃ¡s 28 caracterÃ­sticas calculadas como "
            "medias mÃ³viles, RSI, MACD, medidas de volatilidad y valores "
            "retardados. Estas ayudan al modelo a detectar patrones no "
            "visibles en los precios brutos."
        ),

        # -- Tab 2: Training ------------------------------------------------
        "train_header": "Entrenamiento del modelo",
        "train_warn_no_data": "âš ï¸ Primero cargue los datos en la pestaÃ±a Datos",
        "train_btn": "Entrenar modelo",
        "train_spinner": "Entrenando modelo...",
        "train_complete": "Â¡Entrenamiento completo!",
        "train_success": "âœ… Â¡Modelo entrenado con Ã©xito! Guardado en {path}",
        "train_error": "âŒ Error al entrenar el modelo: {err}",
        "train_history": "Historial de entrenamiento",
        "train_loss_label": "PÃ©rdida entren.",
        "val_loss_label": "PÃ©rdida valid.",
        "train_xlabel": "Ã‰poca",
        "train_ylabel": "PÃ©rdida",
        "train_plot_title": "PÃ©rdida de entrenamiento y validaciÃ³n",
        "train_info": (
            "Al pulsar 'Entrenar modelo' se ejecuta el bucle completo: "
            "selecciÃ³n de caracterÃ­sticas â†’ cÃ¡lculo de objetivo â†’ creaciÃ³n "
            "de secuencias â†’ divisiÃ³n 80/20 entrenamiento/validaciÃ³n â†’ "
            "optimizaciÃ³n por descenso de gradiente. Observe la grÃ¡fica de "
            "pÃ©rdida: ambas curvas deben descender. Si la validaciÃ³n sube "
            "mientras el entrenamiento baja, hay sobreajuste."
        ),

        # -- Tab 3: Predictions ---------------------------------------------
        "pred_header": "Predicciones del modelo",
        "pred_warn_no_model": "âš ï¸ Primero entrene un modelo en la pestaÃ±a Entrenar",
        "pred_vs_actual": "Predicciones vs Real",
        "pred_returns": "**Rendimientos predichos:**",
        "pred_implied": "**Movimientos de precio implÃ­citos:**",
        "pred_signals": "**SeÃ±ales Compra/No-Compra:**",
        "pred_actual_returns": "Rendimientos reales",
        "pred_predicted_returns": "Rendimientos predichos",
        "pred_actual_price": "Precio real",
        "pred_implied_price": "Precio implÃ­cito (segÃºn predicciÃ³n)",
        "pred_actual_signal": "SeÃ±al real",
        "pred_predicted_signal": "SeÃ±al predicha",
        "pred_recent": "Predicciones recientes",
        "pred_error": "âŒ Error al generar predicciones: {err}",
        "pred_col_date": "Fecha",
        "pred_col_price": "Precio real",
        "pred_col_pred": "PredicciÃ³n",
        "pred_col_true": "Valor real",
        "pred_info": (
            "Tras el entrenamiento, el modelo ejecuta un pase hacia adelante "
            "en cada secuencia de entrada. En regresiÃ³n, la salida es el "
            "rendimiento predicho; en clasificaciÃ³n es una probabilidad de "
            "Compra (>0.5) o No-Compra (â‰¤0.5). La grÃ¡fica 'Precio implÃ­cito' "
            "multiplica el precio real por (1 + rendimiento predicho). Estas "
            "son predicciones histÃ³ricas, no pronÃ³sticos futuros reales."
        ),

        # -- Tab 4: Evaluation ----------------------------------------------
        "eval_header": "EvaluaciÃ³n del modelo",
        "eval_warn_no_model": "âš ï¸ Primero entrene un modelo en la pestaÃ±a Entrenar",
        "eval_regression_metrics": "MÃ©tricas de regresiÃ³n",
        "eval_classification_metrics": "MÃ©tricas de clasificaciÃ³n",
        "eval_confusion_matrix": "Matriz de confusiÃ³n",
        "eval_cm_no_buy": "No-Compra",
        "eval_cm_buy": "Compra",
        "eval_cm_title": "Matriz de confusiÃ³n",
        "eval_cm_ylabel": "Etiqueta real",
        "eval_cm_xlabel": "Etiqueta predicha",
        "eval_detailed": "MÃ©tricas detalladas",
        "eval_error": "âŒ Error al evaluar el modelo: {err}",
        "eval_info": (
            "MÃ©tricas de regresiÃ³n: MSE, RMSE, MAE miden el error de "
            "predicciÃ³n (menor es mejor); RÂ² mide la varianza explicada "
            "(1.0 = perfecto). MÃ©tricas de clasificaciÃ³n: la Exactitud es "
            "la fracciÃ³n correcta, pero PrecisiÃ³n, Sensibilidad y F1 son "
            "mÃ¡s informativas porque un modelo ingenuo que siempre diga "
            "'Compra' puede alcanzar ~60% de exactitud. La matriz de "
            "confusiÃ³n muestra conteos TP/FP/FN/TN."
        ),

        # -- Diagnostics panel -----------------------------------------------
        "diag_header": "DiagnÃ³sticos del entrenamiento",
        "diag_verdict": "Veredicto",
        "diag_verdict_healthy": "âœ… Saludable",
        "diag_verdict_overfitting": "âš ï¸ Sobreajuste",
        "diag_verdict_underfitting": "âš ï¸ Infraajuste",
        "diag_verdict_noisy": "âš ï¸ Ruidoso / Inestable",
        "diag_explanation": "ExplicaciÃ³n",
        "diag_suggestions": "Sugerencias",
        "diag_best_epoch": "Mejor Ã©poca",
        "diag_gen_gap": "Brecha de generalizaciÃ³n",

        # -- Multi-task prediction labels ----------------------------------------
        "pred_mt_returns": "**Cabeza de regresiÃ³n â€” Rendimientos predichos:**",
        "pred_mt_signals": "**Cabeza de clasificaciÃ³n â€” SeÃ±ales Compra/No-Compra:**",
        "pred_mt_col_reg": "Rendimiento predicho",
        "pred_mt_col_cls": "Probabilidad de compra",

        # -- Multi-task evaluation labels ----------------------------------------
        "eval_mt_header": "EvaluaciÃ³n multi-tarea",
        "eval_mt_threshold": "Umbral de clasificaciÃ³n",

        # -- Axis labels (plots) -------------------------------------------
        "axis_date": "Fecha",
        "axis_price": "Precio (USD)",
        "axis_returns": "Rendimientos",
        "axis_signal": "SeÃ±al (1=Compra, 0=No-Compra)",

        # -- Tutorial -------------------------------------------------------
        "tut_header": "ğŸ“š Tutorial â€” CÃ³mo funciona esta aplicaciÃ³n",
        "tut_disclaimer": (
            "> **Aviso legal:** Esta aplicaciÃ³n es una herramienta educativa "
            "para explorar el aprendizaje profundo aplicado a series temporales "
            "financieras. Nada en esta guÃ­a ni en la salida de la aplicaciÃ³n "
            "constituye asesoramiento financiero."
        ),
        "tut_s1_title": "1 â€” VisiÃ³n general",
        "tut_s1_body": """
Esta aplicaciÃ³n descarga datos histÃ³ricos de precios del fondo cotizado **GLD**
(ETF de oro), genera un conjunto de caracterÃ­sticas tÃ©cnicas a partir de esos
datos y entrena un modelo de aprendizaje profundo para **predecir movimientos
futuros del precio**.

El flujo de trabajo consta de cuatro pasos, cada uno representado por una
pestaÃ±a en la interfaz:

| PestaÃ±a | PropÃ³sito |
|---------|-----------|
| **ğŸ“Š Datos** | Descargar y explorar precios histÃ³ricos de GLD |
| **ğŸ”§ Entrenar** | Configurar y entrenar una red neuronal |
| **ğŸ“ˆ Predicciones** | Visualizar las predicciones del modelo |
| **ğŸ“‰ EvaluaciÃ³n** | Medir la precisiÃ³n con mÃ©tricas estÃ¡ndar |

La barra lateral izquierda permite configurar cada parÃ¡metro antes de pulsar
*Entrenar modelo*.
""",
        "tut_s2_title": "2 â€” Datos: Carga y exploraciÃ³n",
        "tut_s2_body": """
### CÃ³mo se cargan los datos

Los datos histÃ³ricos de GLD se obtienen mediante **yfinance**, una biblioteca
de Python que descarga datos diarios de mercado de Yahoo Finance. Al pulsar
*Cargar datos*, la aplicaciÃ³n descarga registros diarios OHLCV (Apertura,
MÃ¡ximo, MÃ­nimo, Cierre, Volumen) para el rango de fechas configurado.

### QuÃ© representa cada columna

| Columna | Significado |
|---------|-------------|
| **Open** | Precio de apertura del mercado |
| **High** | Precio mÃ¡s alto del dÃ­a |
| **Low** | Precio mÃ¡s bajo del dÃ­a |
| **Close** | Precio de cierre â€” la referencia mÃ¡s utilizada |
| **Volume** | NÃºmero total de acciones negociadas |
| **Dividends** | Dividendos pagados (generalmente 0 para GLD) |
| **Stock Splits** | Eventos de divisiÃ³n (generalmente 0 para GLD) |

### IngenierÃ­a de caracterÃ­sticas

Los datos OHLCV brutos por sÃ­ solos no son muy informativos para una red
neuronal. La aplicaciÃ³n crea automÃ¡ticamente **28 caracterÃ­sticas adicionales**,
incluyendo:

- **Medias mÃ³viles** (SMA, EMA a 5, 10, 20, 50 dÃ­as)
- **Medidas de volatilidad** â€” desviaciÃ³n estÃ¡ndar mÃ³vil
- **Indicadores de impulso** â€” tasa de cambio del precio
- **RSI (Ãndice de Fuerza Relativa)** â€” sobrecompra / sobreventa (0â€“100)
- **MACD** â€” indicador de impulso de tendencia
- **Ratios de volumen** â€” volumen respecto a promedios recientes
- **Valores retardados** â€” precios y rendimientos de dÃ­as anteriores

Estas ayudan al modelo a detectar **patrones y cambios de rÃ©gimen** no visibles
en los precios brutos.
""",
        "tut_s3_title": "3 â€” Arquitecturas: GRU vs LSTM vs TCN",
        "tut_s3_body": """
### Â¿QuÃ© son las redes neuronales recurrentes (RNN)?

Las redes neuronales estÃ¡ndar tratan cada entrada de forma independiente.
Las **redes neuronales recurrentes** (RNN) procesan *secuencias*: mantienen
un **estado oculto** interno que se actualiza en cada paso temporal,
permitiendo recordar informaciÃ³n anterior.

Esto las hace ideales para **datos de series temporales** como precios
bursÃ¡tiles, donde el orden importa.

### GRU (Unidad Recurrente con Puertas)

Variante moderna (2014) con dos puertas:

- **Puerta de reinicio** â€” decide cuÃ¡nta informaciÃ³n pasada olvidar
- **Puerta de actualizaciÃ³n** â€” decide cuÃ¡nta informaciÃ³n nueva admitir

Las GRU son **mÃ¡s simples y rÃ¡pidas** que las LSTM.

### LSTM (Memoria a Largo-Corto Plazo)

Introducida en 1997, usa tres puertas:

- **Puerta de olvido** â€” quÃ© descartar del estado de celda
- **Puerta de entrada** â€” quÃ© valores nuevos almacenar
- **Puerta de salida** â€” quÃ© parte del estado emitir

Las LSTM tienen un **estado de celda** separado que retiene informaciÃ³n en
**secuencias mÃ¡s largas**.

### TCN (Red Convolucional Temporal)

Una **TCN** sustituye la recurrencia por convoluciones causales 1-D apiladas.
Propiedades clave:

- **Relleno causal** â€” solo ve pasos temporales pasados, nunca el futuro.
- **Filtros dilatados** â€” cada capa duplica la dilataciÃ³n, asÃ­ el campo
  receptivo crece *exponencialmente* con la profundidad.
- **Conexiones residuales** â€” evitan la degradaciÃ³n del gradiente.

Las convoluciones se ejecutan en paralelo (sin dependencia secuencial),
por lo que las TCN **entrenan mÃ¡s rÃ¡pido** que las RNN en GPUs modernas.

### Â¿CuÃ¡ndo elegir cuÃ¡l?

| Criterio | GRU | LSTM | TCN |
|----------|-----|------|-----|
| Velocidad | âš¡ RÃ¡pida | ğŸ¢ MÃ¡s lenta | âš¡âš¡ MÃ¡s rÃ¡pida |
| ParÃ¡metros | Menos | MÃ¡s | Medio |
| Secuencias cortas (â‰¤30) | âœ… Suficiente | âœ… Funciona bien | âœ… Buena |
| Secuencias largas (>60) | âš ï¸ Puede fallar | âœ… Mejor retenciÃ³n | âœ… Gran campo receptivo |
| Datos limitados | âœ… Menos sobreajuste | âš ï¸ MÃ¡s sobreajuste | âœ… Comparte pesos |
| Paralelismo | âŒ Secuencial | âŒ Secuencial | âœ… Totalmente paralela |

**Regla general:** Empiece con GRU. Cambie a LSTM para secuencias largas
o a TCN si la velocidad importa.

### Tipos de tarea

**RegresiÃ³n (Rendimientos)**
- La salida es un nÃºmero continuo: el rendimiento esperado.
- Ejemplo: `0.012` â†’ +1.2 % de aumento esperado.

**ClasificaciÃ³n (Compra / No-Compra)**
- La salida es una probabilidad entre 0 y 1.
- > 0.5 â†’ **Compra** (clase 1); â‰¤ 0.5 â†’ **No-Compra** (clase 0).

**Multi-tarea (RegresiÃ³n + ClasificaciÃ³n)**
- Un backbone compartido alimenta *dos* cabezas de predicciÃ³n simultÃ¡neamente.
- La cabeza de regresiÃ³n predice rendimientos; la de clasificaciÃ³n predice
  seÃ±ales compra/no-compra.
- PÃ©rdida: *L = w_reg Ã— MSE + w_cls Ã— BCEWithLogits*, configurable mediante
  los deslizadores de la barra lateral.
- Ventaja: la representaciÃ³n compartida aprende caracterÃ­sticas mÃ¡s ricas
  al satisfacer ambos objetivos a la vez.
""",
        "tut_s4_title": "4 â€” Horizontes de predicciÃ³n: 1, 5 y 20 dÃ­as",
        "tut_s4_body": """
El **horizonte de predicciÃ³n** es el nÃºmero de dÃ­as de negociaciÃ³n futuros
que el modelo intenta pronosticar.

| Horizonte | Significado | CarÃ¡cter |
|-----------|-------------|----------|
| **1 dÃ­a** | Rendimiento/seÃ±al de maÃ±ana | Corto plazo, mÃ¡s ruido |
| **5 dÃ­as** | Rendimiento de la prÃ³xima semana | Equilibrio |
| **20 dÃ­as** | Rendimiento del prÃ³ximo mes | Largo plazo, mÃ¡s suave |

### Compromisos

- **Horizontes cortos (1 dÃ­a):** capturan movimientos rÃ¡pidos pero estÃ¡n
  dominados por **ruido** (fluctuaciones diarias aleatorias).
- **Horizontes largos (20 dÃ­as):** suavizan el ruido, pero son **mÃ¡s difÃ­ciles
  de predecir** porque intervienen mÃ¡s factores externos.
- **Horizontes medios (5 dÃ­as):** ofrecen un buen equilibrio.

### Consejo prÃ¡ctico

- Si las predicciones son errÃ¡ticas, pruebe un horizonte mÃ¡s largo.
- Si son demasiado planas, pruebe uno mÃ¡s corto.
- Compare mÃ©tricas entre horizontes para encontrar el punto Ã³ptimo.
""",
        "tut_s5_title": "5 â€” ParÃ¡metros configurables",
        "tut_s5_body": """
Cada parÃ¡metro de la barra lateral afecta cÃ³mo aprende el modelo.

---

#### Longitud de secuencia (ventana de observaciÃ³n)
*Barra lateral: 10â€“60, por defecto 20*

NÃºmero de dÃ­as consecutivos que el modelo observa antes de predecir.

| Valor | Efecto |
|-------|--------|
| PequeÃ±o (10) | Menos contexto; mÃ¡s rÃ¡pido; puede perder tendencias |
| Grande (40â€“60) | MÃ¡s contexto; mÃ¡s lento; riesgo de sobreajuste |

---

#### TamaÃ±o oculto
*32â€“128, por defecto 64*

Neuronas internas por capa recurrente â€” controla la capacidad del modelo.

| Valor | Efecto |
|-------|--------|
| PequeÃ±o (32) | MÃ¡s simple; menos sobreajuste; puede infraajustar |
| Grande (128) | MÃ¡s expresivo; mÃ¡s sobreajuste |

---

#### NÃºmero de capas
*1â€“4, por defecto 2*

Capas recurrentes apiladas para patrones jerÃ¡rquicos.

| Valor | Efecto |
|-------|--------|
| 1 | Simple y rÃ¡pido |
| 2 | Buen punto medio |
| 3â€“4 | MÃ¡s potente; necesita mÃ¡s datos |

---

#### Ã‰pocas
*10â€“200, por defecto 50*

Una Ã©poca = el modelo ha visto todas las muestras una vez.

| Valor | Efecto |
|-------|--------|
| Bajo (10â€“20) | Puede infraajustar |
| Medio (30â€“80) | Buen rango |
| Alto (100â€“200) | Riesgo de sobreajuste |

---

#### TamaÃ±o de lote
*16â€“128, por defecto 32*

Muestras procesadas juntas antes de actualizar pesos.

| Valor | Efecto |
|-------|--------|
| PequeÃ±o (16) | Actualizaciones ruidosas; mÃ¡s lento |
| Grande (64â€“128) | Actualizaciones suaves; mÃ¡s rÃ¡pido por Ã©poca |

---

#### Tasa de aprendizaje
*0.0001â€“0.01, por defecto 0.001*

CuÃ¡nto cambian los pesos por lote.

| Valor | Efecto |
|-------|--------|
| Muy baja (0.0001) | Convergencia muy lenta |
| Buena (0.0005â€“0.001) | Aprendizaje estable |
| Muy alta (0.01) | Inestable; puede divergir |
""",
        "tut_s6_title": "6 â€” Entrenamiento: QuÃ© ocurre al pulsar 'Entrenar'",
        "tut_s6_body": """
### El bucle de entrenamiento

1. **SelecciÃ³n de caracterÃ­sticas** â€” 28 caracterÃ­sticas; valores faltantes rellenados.
2. **CÃ¡lculo del objetivo** â€” rendimientos (regresiÃ³n) o etiquetas binarias (clasificaciÃ³n).
3. **CreaciÃ³n de secuencias** â€” ventana deslizante de *Longitud de secuencia* dÃ­as.
4. **DivisiÃ³n entrenamiento / validaciÃ³n** â€” 80 / 20 por defecto.
5. **Bucle de descenso de gradiente** â€” en cada Ã©poca el modelo entrena por lotes
   y luego evalÃºa en el conjunto de validaciÃ³n.

### InterpretaciÃ³n de la grÃ¡fica de historial

- **PÃ©rdida entren.** (azul): error en datos de entrenamiento.
- **PÃ©rdida valid.** (naranja): error en datos no vistos.

| PatrÃ³n | DiagnÃ³stico | AcciÃ³n |
|--------|-------------|--------|
| Ambas descienden | âœ… Buena convergencia | Continuar o parar |
| Entren. â†“ valid. â†‘ | âš ï¸ Sobreajuste | â†“ Ã©pocas / complejidad |
| Ambas altas | âš ï¸ Infraajuste | â†‘ capacidad / Ã©pocas |
| Oscilaciones | âš ï¸ Inestable | â†“ tasa de aprendizaje |
| Plana desde el inicio | âš ï¸ No aprende | â†‘ tasa de aprendizaje |

### Â¿QuÃ© es la "pÃ©rdida" (loss)?

- **RegresiÃ³n:** Error CuadrÃ¡tico Medio (MSE).
- **ClasificaciÃ³n:** EntropÃ­a cruzada binaria.

Menor pÃ©rdida = mejor modelo.
""",
        "tut_s7_title": "7 â€” Predicciones: InterpretaciÃ³n de la salida",
        "tut_s7_body": """
### CÃ³mo se generan las predicciones

El modelo ejecuta un pase hacia adelante en cada secuencia de entrada
(inferencia pura â€” sin cÃ¡lculo de gradientes).

### Salida de regresiÃ³n

- Rendimiento predicho por fecha, superpuesto a los reales.
- **Precio implÃ­cito** = precio real Ã— (1 + rendimiento predicho).

### Salida de clasificaciÃ³n

- Probabilidad > 0.5 â†’ **Compra (1)**; â‰¤ 0.5 â†’ **No-Compra (0)**.
- Puntos azules = real; X rojas = predicho.

### Tabla de predicciones recientes

| Columna | Significado |
|---------|-------------|
| Fecha | DÃ­a de negociaciÃ³n |
| Precio real | Cierre de GLD |
| PredicciÃ³n | Salida bruta del modelo |
| Valor real | Valor objetivo real |

### Advertencias

Las predicciones se hacen sobre datos histÃ³ricos (entrenamiento +
validaciÃ³n), **no** son pronÃ³sticos futuros reales.
""",
        "tut_s8_title": "8 â€” EvaluaciÃ³n: Entender las mÃ©tricas",
        "tut_s8_body": """
### MÃ©tricas de regresiÃ³n

| MÃ©trica | Significado | Buenos valores |
|---------|-------------|----------------|
| **MSE** | Error cuadrÃ¡tico medio | Menor es mejor |
| **RMSE** | âˆšMSE â€” mismas unidades | Menor es mejor |
| **MAE** | Error absoluto medio | Menor es mejor |
| **RÂ²** | Varianza explicada | 1.0 = perfecto; 0 = nivel de la media |

En mercados reales, RÂ² de 0.01â€“0.05 ya puede ser Ãºtil econÃ³micamente.

### MÃ©tricas de clasificaciÃ³n

| MÃ©trica | Significado |
|---------|-------------|
| Exactitud | FracciÃ³n correcta |
| PrecisiÃ³n | De las predicciones Compra, Â¿cuÃ¡ntas correctas? |
| Sensibilidad | De los dÃ­as reales Compra, Â¿cuÃ¡ntos detectados? |
| F1 | Media armÃ³nica de PrecisiÃ³n y Sensibilidad |

#### Matriz de confusiÃ³n

```
                    Predicho
              No-Compra  Compra
Real No-Compra   VN        FP
     Compra      FN        VP
```

**Â¿Por quÃ© la exactitud sola no basta?** Un modelo que siempre diga "Compra"
puede alcanzar ~60 % si el mercado sube el 60 % del tiempo.

| Escenario | MÃ©tricas | InterpretaciÃ³n |
|-----------|----------|----------------|
| Azar | RÂ²â‰ˆ0, Exactitudâ‰ˆ50% | Sin habilidad |
| Ventaja leve | RÂ²â‰ˆ0.01â€“0.05, Exactitudâ‰ˆ52â€“55% | Potencialmente Ãºtil |
| Fuerte (raro) | RÂ²>0.1, F1>0.65 | Verificar sobreajuste |
| Sobreajustado | RÂ²>0.9 solo en entren. | Demasiado bueno |
""",
        "tut_s9_title": "9 â€” Ejemplos prÃ¡cticos y escenarios comunes",
        "tut_s9_body": """
> **Nota:** Estos ejemplos son puramente educativos. NO constituyen
> asesoramiento financiero.

---

**Escenario A â€” PredicciÃ³n positiva con seÃ±al de compra**

Ambos modelos coinciden en que el precio subirÃ¡. La coincidencia aumenta
la confianza, pero no garantiza el resultado. Verifique las mÃ©tricas de
validaciÃ³n.

---

**Escenario B â€” PÃ©rdida de validaciÃ³n sube mientras la de entrenamiento baja**

**Sobreajuste** clÃ¡sico. Reduzca Ã©pocas, complejidad o aumente los datos.

---

**Escenario C â€” Predicciones muy fluctuantes**

Sobreajuste al ruido o tasa de aprendizaje alta. Pruebe â†“ LR,
â†‘ longitud de secuencia u horizonte mÃ¡s largo.

---

**Escenario D â€” El modelo siempre predice el mismo valor**

ColapsÃ³ a la media. â†‘ capacidad, â†‘ Ã©pocas, â†“ LR, o use mÃ¡s datos.

---

**Escenario E â€” Exactitud muy alta en entrenamiento (95 %)**

Probablemente sobreajuste. Una exactitud real superior al 55â€“60 % ya es buena.
""",
        "tut_s10_title": "10 â€” Hoja de referencia rÃ¡pida",
        "tut_s10_body": """
### ConfiguraciÃ³n inicial recomendada

| ParÃ¡metro | Valor |
|-----------|-------|
| Modelo | GRU |
| Tarea | RegresiÃ³n |
| Horizonte | 5 dÃ­as |
| Longitud de secuencia | 20 |
| TamaÃ±o oculto | 64 |
| Capas | 2 |
| Ã‰pocas | 50 |
| TamaÃ±o de lote | 32 |
| Tasa de aprendizaje | 0.001 |

### ComparaciÃ³n rÃ¡pida de arquitecturas

| | GRU | LSTM | TCN |
|-|-----|------|-----|
| Ideal para | Uso general | Secuencias largas | Velocidad |
| ParÃ¡metros | Bajo | Alto | Medio |
| Velocidad | RÃ¡pida | Lenta | MÃ¡s rÃ¡pida |

### Ajustes comunes

| Problema | Pruebe |
|----------|--------|
| Infraajuste | â†‘ TamaÃ±o oculto, â†‘ Capas, â†‘ Ã‰pocas |
| Sobreajuste | â†“ Ã‰pocas, â†“ TamaÃ±o oculto, â†“ Capas, â†‘ Rango de datos |
| PÃ©rdida inestable | â†“ Tasa de aprendizaje |
| Predicciones planas | â†‘ Tasa de aprendizaje, â†‘ TamaÃ±o oculto |
| Predicciones ruidosas | â†‘ Secuencia, â†‘ Horizonte, â†“ Tasa de aprendizaje |
| Entrenamiento lento | â†“ TamaÃ±o oculto, â†“ Capas, â†‘ Lote, TCN o GRU |

### Veredictos del diagnÃ³stico

| Veredicto | Significado | AcciÃ³n |
|-----------|-------------|--------|
| âœ… Saludable | Ambas curvas descienden, brecha estable | Continuar o parar |
| âš ï¸ Sobreajuste | Valid. â†‘ mientras entren. â†“ | â†“ Ã©pocas / complejidad |
| âš ï¸ Infraajuste | Ambas curvas altas y planas | â†‘ capacidad / Ã©pocas |
| âš ï¸ Ruidoso | ValidaciÃ³n oscila | â†“ tasa de aprendizaje, â†‘ lote |
""",
    },
}
